?read_tsv
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
s <- "chr21-29591159-T-A
s
""
"
s <- "chr21-29591159-T-A"
s
ar <- str_split_fixed(s, "-", 4)
ar
ar[1,]
ar[1,][1]
?glimpse
a <- c(5, "a")
class(a)
a
class(33)
print(cat(class(3)))
print(cat())
library(mtcars)
install.packages("mtcars")
help("mtcars")
df <- as.data.frame(mtcars)
View(df)
v1 <- c(1,2,3)
v1
sapply(df, as.character)
v
v <- c(1,2,3)
v[v<3]
1 %in% c(1,2)
runif(10, 0, 100)
?runif
?write.table
?write.delim
??write.delim
knitr::opts_chunk$set(eval = FALSE)
tail(mtcars)
tail(mtcars, 2)
# output: column names: data class: first 10 values.
str(mtcars)
which(is.na(mtcars), arr.ind=TRUE)
?which''
?which
?which
?which
# check if there blank values (no character)
mtcars[mtcars==""]
?View
?View
?dplyr::view
install.packages("caret")
install.package("EnvStats")
install.packages("EnvStats")
knitr::opts_chunk$set(eval = FALSE)
install.packages("missMethods")
install.packages("LOCF")
library(tidyverse)
library(missMethods)
library(MASS)
library(LOCF)
install.packages("LOCF")
install.packages("imputeTS")
install.packages("ggtext")
install.packages("imputeTS")
iris |> dplyr::filter(Species %in% c("setosa", "versicolor")) %>% ggplot(aes(x=Sepal.Width, fill=Species)) +
geom_histogram(bins = 10, position = "dodge") + #гістограма
scale_fill_brewer(palette = "Set2") +
theme_minimal() +
geom_vline(data=sepal_means, aes(xintercept=Sepal.Width.Mean, color=Species),
linetype="dashed")+
theme(legend.position="top")
iris |> dplyr::filter(Species %in% c("setosa", "versicolor")) |>  ggplot(aes(x=Sepal.Width, fill=Species)) +
geom_histogram(bins = 10, position = "dodge") + #гістограма
scale_fill_brewer(palette = "Set2") +
theme_minimal() +
geom_vline(data=sepal_means, aes(xintercept=Sepal.Width.Mean, color=Species),
linetype="dashed")+
theme(legend.position="top")
library(tidyverse)
iris |> dplyr::filter(Species %in% c("setosa", "versicolor")) |>  ggplot(aes(x=Sepal.Width, fill=Species)) +
geom_histogram(bins = 10, position = "dodge") + #гістограма
scale_fill_brewer(palette = "Set2") +
theme_minimal() +
geom_vline(data=sepal_means, aes(xintercept=Sepal.Width.Mean, color=Species),
linetype="dashed")+
theme(legend.position="top")
sepal_means <- iris %>% group_by(Species) %>% summarise(Sepal.Width.Mean = mean(Sepal.Width))
iris |> dplyr::filter(Species %in% c("setosa", "versicolor")) |>  ggplot(aes(x=Sepal.Width, fill=Species)) +
geom_histogram(bins = 10, position = "dodge") + #гістограма
scale_fill_brewer(palette = "Set2") +
theme_minimal() +
geom_vline(data=sepal_means, aes(xintercept=Sepal.Width.Mean, color=Species),
linetype="dashed")+
theme(legend.position="top")
sepal_means <- iris |>  group_by(Species) |>  summarise(Sepal.Width.Mean = mean(Sepal.Width))
iris |> dplyr::filter(Species %in% c("setosa", "versicolor")) |>  ggplot(aes(x=Sepal.Width, fill=Species)) +
geom_histogram(bins = 10, position = "dodge") + #гістограма
scale_fill_brewer(palette = "Set2") +
theme_minimal() +
geom_vline(data=sepal_means, aes(xintercept=Sepal.Width.Mean, color=Species),
linetype="dashed")+
theme(legend.position="top")
pak::pak(
c("arrow", "babynames", "curl", "duckdb", "gapminder",
"ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman",
"leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins",
"repurrrsive", "tidymodels", "tidyverse", "writexl")
)
pak::pak(
c("arrow", "babynames", "curl", "duckdb", "gapminder",
"ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman",
"leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins",
"repurrrsive", "tidymodels", "tidyverse", "writexl")
)
pak::pak(
c("arrow", "babynames", "curl", "duckdb", "gapminder",
"ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman",
"leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins",
"repurrrsive", "tidymodels", "tidyverse", "writexl")
)
pak::pak(
c("arrow", "babynames", "curl", "duckdb", "gapminder",
"ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman",
"leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins",
"repurrrsive", "tidymodels", "tidyverse", "writexl")
)
pak::pak(
c("arrow", "babynames", "curl", "duckdb", "gapminder",
"ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman",
"leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins",
"repurrrsive", "tidymodels", "tidyverse", "writexl")
)
tempdir()
usethis::edit_r_environ()
tempdir()
pak::pak(
c("arrow", "babynames", "curl", "duckdb", "gapminder",
"ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman",
"leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins",
"repurrrsive", "tidymodels", "tidyverse", "writexl")
)
tempdir()
tempdir()
?tempdir
tempdir()
system("Desktop/teaching_n_learning/ucu2024_bioinformatics_of_genomic_variation_analysis/data/week5_2_work/bash_example.sh")
reticulate::repl_python()
library(tidyverse)
library(palmerpenguins)
library(ggthemes)
library(stringr)
1 / 200 * 30
(59 + 73 + 2) / 3
sin(pi/2)
library(reticulate)
reticulate::repl_python()
reticulate::repl_python()
cars <- mtcars
View(cars)
1 / 200 * 30
(59 + 73 + 2) / 3
sin(pi/2)
1+2
reticulate::repl_python()
setwd("~/Desktop/teaching_n_learning/2025Q1-DAAD-sc-spatial/Module_1/S05/")
install.packages('tidytext')
install.packages('stopwords')
install.packages('wordcloud')
install.packages('wordcloud2')
install.packages('topicmodels')
install.packages('igraph')
install.packages('ggraph')
install.packages('Rcpp')
install.packages('lubridate')
install.packages('textdata')
install.packages('htmlwidgets')
getwd()
library(tidytext) #To perform different operations with text data
library(tidyverse) #General data cleaning
library(stopwords) #Stopwords dictionary
library(wordcloud) #One way to plot wordclouds
library(wordcloud2) #Alternative way of producing wordcloud
library(lubridate) #To work with dates
library(topicmodels) #For topic modelling
install.packages('topicmodels')
library(tidytext) #To perform different operations with text data
library(tidyverse) #General data cleaning
library(stopwords) #Stopwords dictionary
library(wordcloud) #One way to plot wordclouds
library(wordcloud2) #Alternative way of producing wordcloud
library(lubridate) #To work with dates
library(topicmodels) #For topic modelling
install.packages('topicmodels')
install.packages('topicmodels')
library(tidytext) #To perform different operations with text data
library(tidyverse) #General data cleaning
library(stopwords) #Stopwords dictionary
library(wordcloud) #One way to plot wordclouds
library(wordcloud2) #Alternative way of producing wordcloud
library(lubridate) #To work with dates
library(topicmodels) #For topic modelling
library(igraph) #To plot network of words
library(ggraph) #To plot network of words
library(Rcpp) #technical package to plot graph
library(textdata)
library(htmlwidgets) #To save wordcloud2 as html file
articles <- read_csv('data.csv')
articles_txt <- readLines("data.txt")
articles_txt_df <- tibble(lines = 1:12467, text = articles_txt)
articles_txt_df <- tibble(lines = 1:22573, text = articles_txt)
remove(articles_txt, articles_txt_df)
names(articles)
View(articles)
?unnest_tokens
articles_unnested <- articles |> unnest_tokens(word, claim_reviewed, to_lower = TRUE)
stop_words <- stopwords(language = "en",source = "marimo")
stop_words
stop_words_df <- tibble(lines = 1:237, word = stop_words)
articles_cleaned <- articles_unnested |> anti_join(stop_words_df, by = "word")
stop_words_countries <- c(stop_words, 'europe', 'russia', 'eu', 'russian', 'united', 'states',
'american', 'usa', "syria", "ukraine", "kyiv", 'donbass',
'crimea', 'belarus', 'poland', 'western', 'us', 'ukrainian',
'eastern', 'west', 'donbas', 'moscow', 'european', 'germany', 'georgia', 'ukrainians', 'Union', 'belarusain')
stop_words_countries_df <- tibble(lines = 1:265, word = stop_words_countries)
articles_no_countries <- articles_unnested |> anti_join(stop_words_countries_df, by = "word")
top_words <- articles_cleaned |> count(word) |> arrange(desc(n))
top_words_no_countries <- articles_no_countries |> count(word) |> arrange(desc(n))
wordcloud(
words = top_words$word,
freq = top_words$n,
max.words = 100,
random.order = FALSE
)
wordcloud(
words = top_words_no_countries$word,
freq = top_words_no_countries$n,
max.words = 100,
random.order = TRUE
)
wordcloud_2 <- wordcloud2(top_words)
saveWidget(wordcloud_2, "wordcloud_2.html")
browseURL( "wordcloud_2.html")
wordcloud_2_no_count <- wordcloud2(top_words_no_countries)
saveWidget(wordcloud_2_no_count, "wordcloud_2_no_count.html")
browseURL( "wordcloud_2_no_count.html")
top_words_10 <- top_words |> top_n(10)
top_words_no_countries_10 <- top_words_no_countries |> top_n(10)
ggplot(top_words_10, aes(x = fct_reorder(word,n), y = n)) +
geom_col() +
coord_flip()
articles_no_countries <- articles_no_countries |>
left_join(get_sentiments("nrc"), by = "word") |>
rename(sentiment_nrc = sentiment) |>
left_join(get_sentiments('bing'), by = 'word') |>
rename(sentiment_bing = sentiment) |>
left_join(get_sentiments('afinn'), by = 'word') |>
rename(sentiment_afinn = value)
unique(articles_no_countries$sentiment_nrc) #10 different sentiments
unique(articles_no_countries$sentiment_bing) #positive or negative sentiments
unique(articles_no_countries$sentiment_afinn) #Numeric values
sentiment_count <- articles_no_countries |>
group_by(sentiment_nrc) |>
summarize(count = n()) |>
filter(!is.na(sentiment_nrc))
ggplot(sentiment_count, aes(x = fct_reorder(sentiment_nrc,count), y = count)) +
geom_col() +
coord_flip()
sentiment_count <- articles_no_countries |>
group_by(sentiment_bing) |>
summarize(count = n()) |>
filter(!is.na(sentiment_bing))
ggplot(sentiment_count, aes(x = fct_reorder(sentiment_bing,count), y = count)) +
geom_col() +
coord_flip()
summary_word_sentiment <- articles_no_countries |>
group_by(sentiment_bing, word) |>
summarize(count = n()) |>
filter(!is.na(sentiment_bing)) |>
top_n(10)
ggplot(summary_word_sentiment, aes(x = fct_reorder(word,count), y = count, fill = sentiment_bing)) +
geom_col() +
coord_flip()
mean(articles_no_countries$sentiment_afinn, na.rm = TRUE)
articles_no_countries <- articles_no_countries |> mutate(month = month(claim_published),
year = year(claim_published))
##Grouping by the month and year & calculating the average level of sentiment for each month
summary_by_date <- articles_no_countries |> group_by(month, year) |> summarise(mean_sent = mean(sentiment_afinn, na.rm = TRUE))
summary_by_date <- articles_no_countries |> group_by(month, year) |> summarise(mean_sent = mean(sentiment_afinn, na.rm = TRUE))
summary_by_date$date<-as.Date(with(summary_by_date,paste(year,month,"01",sep="-")),"%Y-%m-%d")
ggplot(summary_by_date, aes(x=date, y=mean_sent)) +
geom_line() +
xlab("")+scale_x_date(date_labels = "%m-%Y", date_breaks = "4 month")
articles_dtm <- articles_cleaned |>
count(word, `...1`) |>
cast_dtm(`...1`, word, n) |>
as.matrix()
articles_dtm[1:6, 1000:1010]
lda_model <- LDA(
articles_dtm,
k = 2,
method =
"Gibbs"
,
control = list(seed = 16)
)
topics <- tidy(lda_model, matrix = "beta")
top_terms <- topics |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
top_terms |>
mutate(term = reorder_within(term, beta, topic)) |>
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
beta_wide <- topics |>
mutate(topic = paste0("topic", topic)) |>
pivot_wider(names_from = topic, values_from = beta) |>
filter(topic1 > .001 | topic2 > .001) |>
mutate(log_ratio = log2(topic2 / topic1))
beta_wide |>
group_by(direction = log_ratio > 0) |>
slice_max(abs(log_ratio), n = 10) |>
ungroup() |>
mutate(term = reorder(term, log_ratio)) |>
ggplot(aes(log_ratio, term)) +
geom_col() +
labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)
articles_bigrams <- articles |> unnest_tokens(word, claim_reviewed, token = "ngrams", n = 2, to_lower = TRUE)
View(articles_bigrams)
articles_bigrams |>
count(word, sort = TRUE)
articles_bigrams_separated <- articles_bigrams |>
separate(word, c("word1", "word2"), sep = " ")
bigrams_filtered <- articles_bigrams_separated |>
filter(!word1 %in% stop_words_df$word) |>
filter(!word2 %in% stop_words_df$word)
bigram_counts <- bigrams_filtered |>
count(word1, word2, sort = TRUE)
View(bigram_counts)
bigrams_filtered |>
filter(word1 == "us") |>
count(word2, sort = TRUE)
not_words <- articles_bigrams_separated |>
filter(word1 == "not") |>
inner_join(get_sentiments('afinn'), by = c(word2 = "word")) |>
count(word2, value, sort = TRUE)
not_words |>
mutate(contribution = n * value) |>
arrange(desc(abs(contribution))) |>
head(20) |>
mutate(word2 = reorder(word2, contribution)) |>
ggplot(aes(n * value, word2, fill = n * value > 0)) +
geom_col(show.legend = FALSE) +
labs(x = "Sentiment value * number of occurrences",
y = "Words preceded by \"not\"")
not_words <- not_words |> mutate(contribution = n*value)
sum(not_words$contribution)
bigram_graph <- bigram_counts |>
filter(n > 20) |>
graph_from_data_frame()
bigram_graph
set.seed(321)
?ggraph
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
